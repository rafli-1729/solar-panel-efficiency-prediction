{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":83964,"databundleVersionId":9381308,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library and Configuration","metadata":{}},{"cell_type":"code","source":"# --- System & Environment Configuration --- \nimport os\nimport random\nfrom warnings import filterwarnings\n\n# Matikan log TensorFlow yang tidak perlu\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n# Abaikan warnings\nfilterwarnings(\"ignore\")\n\n\n# --- Core Library ---\nimport chardet\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\npd.set_option('display.max_colwidth', None)\n\n# --- Visualization ---\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# --- Scikit Learn ---\n# Model Selection & Evaluation\nfrom sklearn.model_selection import (\n    KFold, \n    cross_validate, \n    cross_val_score, \n    train_test_split\n)\nfrom sklearn.metrics import (\n    r2_score, \n    mean_squared_error,\n    mean_absolute_error,\n)\nfrom sklearn.base import (\n    BaseEstimator,\n    TransformerMixin\n)\n    \n# Preprocessing & Pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import (\n    OneHotEncoder, \n    StandardScaler, \n    RobustScaler,\n)\nfrom sklearn.impute import (\n    SimpleImputer\n)\nfrom sklearn.compose import (\n    TransformedTargetRegressor\n)\n\n# ML Models\nimport optuna\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Possible Deep Learning Frameworks\nimport tensorflow as tf\n\nROOT_PATH = Path('/kaggle/input/preliminary-round-dac-prs-2024/dataset/')\nTRAIN_PATH = ROOT_PATH/'train.csv'\nWEATHER_PATH = ROOT_PATH/'Weather.csv'\nTEST_PATH = ROOT_PATH/'test.csv'\nMETADATA_PATH = ROOT_PATH/'metadata.csv'\nSAMPLE_SUBMISSION = ROOT_PATH/'sample_submission.csv'\nSOLAR_IRRADIANCE = ROOT_PATH/'solar-irradiance'\n\nTARGET = '% Baseline'\nN_SPLITS=5\n\ndef set_seed(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nSEED = 42\nset_seed(SEED)\n\nprint('library and configuration ready!')","metadata":{"_uuid":"2c5c51fd-cfa8-4ee3-b2d6-29820eb44a71","_cell_guid":"52ebb208-323f-482a-b0d9-9c3488a1a240","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-23T12:56:20.679998Z","iopub.execute_input":"2025-11-23T12:56:20.680429Z","iopub.status.idle":"2025-11-23T12:56:20.706776Z","shell.execute_reply.started":"2025-11-23T12:56:20.680397Z","shell.execute_reply":"2025-11-23T12:56:20.705815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read data to detect encoding\nwith open(METADATA_PATH, 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\n\nprint(f\"Detected encoding: {result['encoding']}\")\n\n# read with detected encoding\nmetadata = pd.read_csv(METADATA_PATH, encoding=result['encoding'], sep=';', index_col='No.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:36:52.470185Z","iopub.execute_input":"2025-11-23T12:36:52.470473Z","iopub.status.idle":"2025-11-23T12:36:52.575692Z","shell.execute_reply.started":"2025-11-23T12:36:52.470447Z","shell.execute_reply":"2025-11-23T12:36:52.574708Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"class SolarLunarTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, features):\n        self.features = features\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        \n        for col in self.features:\n            # 1. Ekstrak menit (Row dengan NaT akan menghasilkan NaN di sini)\n            minutes = X_copy[col].dt.hour * 60 + X_copy[col].dt.minute\n            \n            # 2. Buat Masking: Kita tandai mana baris yang NaN\n            is_nan = minutes.isna()\n            \n            # 3. Fillna sementara dengan 0 agar np.sin tidak mengeluarkan Warning\n            # Kita pakai .fillna(0) pada copy series agar tidak merusak data asli\n            minutes_safe = minutes.fillna(0)\n            \n            # 4. Normalisasi & Kalkulasi\n            normalized_time = minutes_safe / 1440.0\n            \n            sin_values = np.sin(2 * np.pi * normalized_time)\n            cos_values = np.cos(2 * np.pi * normalized_time)\n            \n            # 5. PENTING: Kembalikan nilai NaN ke posisi aslinya menggunakan np.where\n            # Logic: Jika is_nan True, isi dengan np.nan, jika False pakai hasil hitungan\n            X_copy[f'{col}_sin'] = np.where(is_nan, np.nan, sin_values)\n            X_copy[f'{col}_cos'] = np.where(is_nan, np.nan, cos_values)\n            \n            # Opsional: Drop kolom asli jika mau hemat memori\n            # X_copy.drop(columns=[col], inplace=True)\n            \n        return X_copy[[c for c in X_copy.columns if '_sin' in c or '_cos' in c]]\n\n# To make single time format for merging\ndef make_timestamp(df) -> pd.Series:  \n    if \"date_time\" not in df.columns:\n        raise KeyError(\"Column 'date_time' not found in dataframe.\")\n        \n    dt = pd.to_datetime(df[\"date_time\"])\n    time = dt.dt.strftime(\"%b %-d, %Y %-I\")\n    suffix = dt.dt.strftime(\"%p\").str.lower()\n    \n    timestamp = time + suffix\n    \n    return pd.to_datetime(timestamp)\n\n# To convert multiple columns in dataframe to single datetime\ndef columns_to_datetime(df) -> pd.Series:\n    return pd.to_datetime(\n        df[['Year', 'Month', 'Day', 'Hour', 'Minute']]\n    )\n\n# To merge main dataset to multiple feature datasets\ndef merge_with_features(df, *feature_dfs) -> pd.DataFrame:\n    if \"Timestamp\" not in df.columns:\n        raise KeyError(\"Column 'Timestamp' not found in dataframe.\")\n\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], format=\"mixed\")\n        \n    for feature in feature_dfs:\n        df = df.merge(feature, on=\"Timestamp\", how=\"left\")\n        \n    return df\n\n# To load multiple csv file by year\ndef load_yearly_csv(path, prefix, years) -> pd.DataFrame:\n    return pd.concat(\n        [pd.read_csv(path / f\"{prefix}_{year}.csv\") for year in years],\n        ignore_index=True\n    )\n\n# To convert date time format to submission format\ndef convert_to_submission(df) -> pd.DataFrame:\n    if \"Timestamp\" not in df.columns:\n        raise KeyError(\"Column 'Timestamp' not found in dataframe.\")\n        \n    dt = pd.to_datetime(df[\"Timestamp\"])\n    time = dt.dt.strftime(\"%b %-d, %Y %-I\")\n    suffix = dt.dt.strftime(\"%p\").str.lower()\n\n    df['Timestamp'] = time + suffix\n    return df\n\n# To plot target\ndef check_transformations(series, bins=50):\n    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n    \n    # --- Original Data ---\n    sns.histplot(series, bins=bins, kde=True, ax=axes[0], color='skyblue', edgecolor='black')\n    axes[0].set_title(f'Original\\n(Skew: {series.skew():.2f})', fontsize=14)\n    \n    # --- Log Transformation ---\n    series_log = np.log(series[series > 0]) \n    sns.histplot(series_log, bins=bins, kde=True, ax=axes[1], color='salmon', edgecolor='black')\n    axes[1].set_title(f'Log Transform\\n(Skew: {series_log.skew():.2f})', fontsize=14)\n    \n    # --- Square Root Transformation ---\n    series_sqrt = np.sqrt(series)\n    sns.histplot(series_sqrt, bins=bins, kde=True, ax=axes[2], color='lightgreen', edgecolor='black')\n    axes[2].set_title(f'Square Root Transform\\n(Skew: {series_sqrt.skew():.2f})', fontsize=14)\n\n    plt.suptitle(f'{TARGET} Distribution', fontsize=18, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n# To plot correlation coefficient for each feature with each others\ndef plot_numerical_correlation(X: pd.DataFrame, numerical_features: list) -> None:\n    \"\"\"\n    Computes and visualizes the Pearson correlation matrix for numerical features\n    using a heatmap.\n\n    Args:\n        X (pd.DataFrame): Input dataframe.\n        numerical_features (list): List of numerical column names to correlate.\n    \"\"\"\n    # Calculate the pairwise correlation of columns, excluding NA/null values\n    correlation_matrix = X[numerical_features].corr(method='pearson')\n    \n    plt.figure(figsize=(15, 12))\n    sns.heatmap(\n        correlation_matrix,\n        cmap='coolwarm',      # Diverging colormap (Red for pos, Blue for neg)\n        vmin=-1, vmax=1,      # Anchor the colormap range\n        center=0,             # Center the colormap at 0\n        linewidths=.5,        \n        cbar_kws={'label': 'Correlation Coefficient'}\n    )\n    \n    plt.title('Numerical Features Correlation Heatmap (Pearson)', \n              fontsize=14, fontweight='bold')\n    \n    plt.xticks(rotation=45, ha='right', fontsize=10)\n    plt.yticks(rotation=0, fontsize=10)\n    plt.tight_layout()\n    plt.show()\n\ndef compare_similar_features(df, cols):\n    \"\"\"\n    Membandingkan dua fitur dengan Boxplot berdampingan dan menghitung korelasi.\n    Args:\n        df (pd.DataFrame): Dataframe sumber.\n        cols (list): List berisi dua nama kolom, e.g. ['tempC', 'Temperature']\n    \"\"\"\n    # Validasi input biar gak error\n    if len(cols) != 2:\n        print(\"Error: List must be consist of two columns.\")\n        return\n    \n    col1, col2 = cols[0], cols[1]\n    \n    # Pearson Correlation\n    corr_val = df[col1].corr(df[col2])\n\n    # Plot Setup (1 Row, 2 Columns)\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Plot 1: First Boxplot\n    sns.boxplot(y=df[col1], ax=axes[0], color='skyblue', width=0.4)\n    axes[0].set_title(f'Distribusi: {col1}', fontsize=12, fontweight='bold')\n    axes[0].grid(True, axis='y', alpha=0.3)\n\n    # Plot 2: Second Boxplot\n    sns.boxplot(y=df[col2], ax=axes[1], color='salmon', width=0.4)\n    axes[1].set_title(f'Distribusi: {col2}', fontsize=12, fontweight='bold')\n    axes[1].grid(True, axis='y', alpha=0.3)\n\n    # Main title and correlation score\n    plt.suptitle(f'Perbandingan Fitur: {col1} vs {col2}\\nCorrelation Score: {corr_val:.4f}', \n                 fontsize=16, y=1.05)\n    \n    plt.tight_layout()\n    plt.show()\n\n# To add year, month, and day to solar and lunar features\ndef merge_column_with_timestamp(df, column) -> pd.Series:\n    date_str = df['Timestamp'].dt.strftime('%Y-%m-%d')\n    combined_str = date_str + ' ' + df[column]\n\n    return pd.to_datetime(combined_str, errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:36:52.576728Z","iopub.execute_input":"2025-11-23T12:36:52.577032Z","iopub.status.idle":"2025-11-23T12:36:52.601248Z","shell.execute_reply.started":"2025-11-23T12:36:52.577001Z","shell.execute_reply":"2025-11-23T12:36:52.600405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"markdown","source":"## Solar Irradiance features","metadata":{}},{"cell_type":"code","source":"# Load 4 year solar irradiance datasets\nsolar_irradiance = load_yearly_csv(SOLAR_IRRADIANCE, 'Solar_Irradiance', range(2014, 2018))\n\n# Make timestamp from 5 columns in solar_irradiance: ['Year', 'Month', 'Day', 'Hour', 'Minute']\nsolar_irradiance['date_time'] = columns_to_datetime(solar_irradiance)\nsolar_irradiance['Timestamp'] = make_timestamp(solar_irradiance)\n\n# Drop unnecessary columns\nsolar_irradiance.drop(columns = ['date_time', 'Year', 'Month',\n                                 'Day', 'Hour', 'Minute'], inplace=True)\n\nsolar_irradiance.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:36:52.602474Z","iopub.execute_input":"2025-11-23T12:36:52.602805Z","iopub.status.idle":"2025-11-23T12:36:56.305351Z","shell.execute_reply.started":"2025-11-23T12:36:52.602777Z","shell.execute_reply":"2025-11-23T12:36:56.304419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solar_metadata = metadata[metadata['Variable Name'].isin(solar_irradiance.columns)].reset_index(drop=True)\nsolar_metadata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:36:56.307477Z","iopub.execute_input":"2025-11-23T12:36:56.307805Z","iopub.status.idle":"2025-11-23T12:36:56.318056Z","shell.execute_reply.started":"2025-11-23T12:36:56.307784Z","shell.execute_reply":"2025-11-23T12:36:56.317356Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Weather features","metadata":{}},{"cell_type":"code","source":"# Load and make timestamp column for weather datasets\nweather = pd.read_csv(WEATHER_PATH)\nweather['Timestamp'] = make_timestamp(weather)\n\n# Drop unnecessary column\nweather.drop(columns='date_time', inplace=True)\n\n# Merge timestamp for solar and lunar features\nsolar_lunar = ['sunrise', 'sunset', 'moonrise', 'moonset']\n\nfor feature in solar_lunar:\n    weather[feature] = merge_column_with_timestamp(weather, feature)\n\nweather.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:36:56.319091Z","iopub.execute_input":"2025-11-23T12:36:56.319411Z","iopub.status.idle":"2025-11-23T12:37:00.623408Z","shell.execute_reply.started":"2025-11-23T12:36:56.319383Z","shell.execute_reply":"2025-11-23T12:37:00.622536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weather_metadata = metadata[metadata['Variable Name'].isin(weather.columns)].reset_index(drop=True)\nweather_metadata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:00.624393Z","iopub.execute_input":"2025-11-23T12:37:00.624688Z","iopub.status.idle":"2025-11-23T12:37:00.635117Z","shell.execute_reply.started":"2025-11-23T12:37:00.624667Z","shell.execute_reply":"2025-11-23T12:37:00.634183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train and Test Merging","metadata":{}},{"cell_type":"code","source":"# Load the train and test dataset\ntest  = pd.read_csv(TEST_PATH)\ntrain = pd.read_csv(TRAIN_PATH)\n\n# Define feature datasets\nfeatures = (weather, solar_irradiance)\n\n# Merge main dataset with features\ntrain_raw = merge_with_features(train, *features)\ntest_raw = merge_with_features(test, *features)\n\n# Print train and test shape\nprint('Train shape:', train_raw.shape)\nprint('Test shape:', test_raw.shape) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:00.636241Z","iopub.execute_input":"2025-11-23T12:37:00.636585Z","iopub.status.idle":"2025-11-23T12:37:02.653683Z","shell.execute_reply.started":"2025-11-23T12:37:00.636563Z","shell.execute_reply":"2025-11-23T12:37:02.652806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_metadata = metadata[metadata['Variable Name'].isin(train.columns)].reset_index(drop=True)\ntrain_metadata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:02.654463Z","iopub.execute_input":"2025-11-23T12:37:02.654679Z","iopub.status.idle":"2025-11-23T12:37:02.663793Z","shell.execute_reply.started":"2025-11-23T12:37:02.654661Z","shell.execute_reply":"2025-11-23T12:37:02.662871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_raw.info()\ntest_raw.info() # Target feature: [% Baseline]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:02.664756Z","iopub.execute_input":"2025-11-23T12:37:02.665097Z","iopub.status.idle":"2025-11-23T12:37:02.727212Z","shell.execute_reply.started":"2025-11-23T12:37:02.665068Z","shell.execute_reply":"2025-11-23T12:37:02.725998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_raw['Cloud Type'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:02.730417Z","iopub.execute_input":"2025-11-23T12:37:02.730708Z","iopub.status.idle":"2025-11-23T12:37:02.739937Z","shell.execute_reply.started":"2025-11-23T12:37:02.730686Z","shell.execute_reply":"2025-11-23T12:37:02.738932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('===================================Train head===================================\\n', train_raw.head())\nprint('\\n===================================Test head===================================\\n', test_raw.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:02.740756Z","iopub.execute_input":"2025-11-23T12:37:02.741005Z","iopub.status.idle":"2025-11-23T12:37:02.778310Z","shell.execute_reply.started":"2025-11-23T12:37:02.740986Z","shell.execute_reply":"2025-11-23T12:37:02.777232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from statsmodels.tools.tools import add_constant\n# from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# X = train_df[numerical_features].copy()\n\n# # Delete missing values\n# X = (X.dropna()\n#      # .drop(columns=['WindChillC']) # Opsional: delete highhest VIF value\n#     )\n\n# # intercept\n# X_with_const = add_constant(X)\n\n# # 4. Hitung VIF\n# vif_data = pd.DataFrame()\n# vif_data[\"feature\"] = X_with_const.columns\n# vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i)\n#                    for i in range(len(X_with_const.columns))]\n\n# # 5. Tampilkan hasil (urutkan dari yang terbesar)\n# print(vif_data.sort_values(by=\"VIF\", ascending=False))","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-23T12:37:02.779238Z","iopub.execute_input":"2025-11-23T12:37:02.779572Z","iopub.status.idle":"2025-11-23T12:37:02.799198Z","shell.execute_reply.started":"2025-11-23T12:37:02.779544Z","shell.execute_reply":"2025-11-23T12:37:02.798445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# For a flexible exploration, we can separate features and target\nX_raw=train_raw.drop(columns=[TARGET])\ny_raw=train_raw[TARGET]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:02.800225Z","iopub.execute_input":"2025-11-23T12:37:02.800543Z","iopub.status.idle":"2025-11-23T12:37:02.831342Z","shell.execute_reply.started":"2025-11-23T12:37:02.800521Z","shell.execute_reply":"2025-11-23T12:37:02.830247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_transformations(y_raw)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:02.832300Z","iopub.execute_input":"2025-11-23T12:37:02.832642Z","iopub.status.idle":"2025-11-23T12:37:04.261666Z","shell.execute_reply.started":"2025-11-23T12:37:02.832620Z","shell.execute_reply":"2025-11-23T12:37:04.260787Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Akan dilakukan square root transform","metadata":{}},{"cell_type":"code","source":"numerical_features = X_raw.select_dtypes(np.number).columns\n\nplot_numerical_correlation(X_raw, numerical_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:29:45.559607Z","iopub.execute_input":"2025-11-23T13:29:45.560666Z","iopub.status.idle":"2025-11-23T13:29:46.420987Z","shell.execute_reply.started":"2025-11-23T13:29:45.560633Z","shell.execute_reply":"2025-11-23T13:29:46.419897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ada beberapa fitur yang berulang seperti `Temperature` dan `tempC` juga antara `DewPointC` dan `Dew Point`. Mengenai eliminasi fitur fitur tersebut akan dipertimbangkan nantinya ","metadata":{}},{"cell_type":"markdown","source":"## Exploration with similar features","metadata":{}},{"cell_type":"code","source":"compare_similar_features(train_raw, ['Temperature', 'tempC'])\ncompare_similar_features(train_raw, ['pressure', 'Pressure'])\ncompare_similar_features(train_raw, ['DewPointC', 'Dew Point'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:05.120839Z","iopub.execute_input":"2025-11-23T12:37:05.121187Z","iopub.status.idle":"2025-11-23T12:37:06.336597Z","shell.execute_reply.started":"2025-11-23T12:37:05.121159Z","shell.execute_reply":"2025-11-23T12:37:06.335687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\n\n# Scatter plot GHI vs Target\nsns.scatterplot(x=train_raw['GHI'], y=train_raw[TARGET], alpha=0.3)\nplt.title(f\"Hubungan GHI vs {TARGET}\")\nplt.xlabel(\"GHI (Irradiance)\")\nplt.ylabel(\"Target (Output)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:06.337525Z","iopub.execute_input":"2025-11-23T12:37:06.337846Z","iopub.status.idle":"2025-11-23T12:37:06.736304Z","shell.execute_reply.started":"2025-11-23T12:37:06.337825Z","shell.execute_reply":"2025-11-23T12:37:06.735131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  >>> SETTINGS <<<\n# Ensure these columns exist. Assuming 'GNI' meant 'DNI' (Direct Normal Irradiance)\nTARGET = '% Baseline'\nFEATURES = ['GHI', 'DNI', 'Solar Zenith Angle', 'tempC']\n\ndef astronomical_investigation(df):\n    data = df.copy()\n    data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n    \n    # Extract time components\n    data['Month'] = data['Timestamp'].dt.month\n    data['Hour'] = data['Timestamp'].dt.hour\n    \n    # Create canvas\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    fig.suptitle('Astronomical & Physical Data Inspection', fontsize=16)\n\n    # ---------------------------------------------------------\n    # PLOT 1: Monthly Production (Seasonality Check)\n    # Purpose: Detect Hemisphere (North vs South)\n    # ---------------------------------------------------------\n    monthly_stats = data.groupby('Month')[[TARGET, 'GHI']].mean()\n    \n    ax1 = axes[0, 0]\n    ax1_right = ax1.twinx()\n    \n    sns.lineplot(data=monthly_stats, x=monthly_stats.index, y=TARGET, \n                 ax=ax1, color='blue', marker='o', label='Energy Output', linewidth=2)\n    sns.lineplot(data=monthly_stats, x=monthly_stats.index, y='GHI', \n                 ax=ax1_right, color='orange', marker='s', label='GHI (Irradiance)', linestyle='--')\n    \n    ax1.set_title(\"1. Monthly Average: Seasonality & Hemisphere\")\n    ax1.set_ylabel(\"Energy Output\")\n    ax1_right.set_ylabel(\"GHI\")\n    ax1.legend(loc='upper left')\n    ax1_right.legend(loc='upper right')\n    ax1.grid(True, alpha=0.3)\n\n    # ---------------------------------------------------------\n    # PLOT 2: Solar Noon Verification (Timezone Check)\n    # Purpose: Check if peak sun aligns with 12:00 PM\n    # ---------------------------------------------------------\n    hourly_stats = data.groupby('Hour')[[TARGET, 'GHI', 'DNI']].mean()\n    \n    ax2 = axes[0, 1]\n    # Normalize values to 0-1 for easy comparison\n    hourly_norm = (hourly_stats - hourly_stats.min()) / (hourly_stats.max() - hourly_stats.min())\n    \n    sns.lineplot(data=hourly_norm, x=hourly_norm.index, y=TARGET, ax=ax2, label='Energy Output')\n    sns.lineplot(data=hourly_norm, x=hourly_norm.index, y='GHI', ax=ax2, label='GHI')\n    sns.lineplot(data=hourly_norm, x=hourly_norm.index, y='DNI', ax=ax2, label='DNI (Direct)', linestyle=':')\n    \n    # Find peak hour\n    peak_ghi_hour = hourly_stats['GHI'].idxmax()\n    ax2.axvline(peak_ghi_hour, color='red', linestyle='--', alpha=0.5, label=f'Peak Sun ({peak_ghi_hour}:00)')\n    \n    ax2.set_title(f\"2. Diurnal Cycle: Solar Noon Check (Peak at {peak_ghi_hour}:00?)\")\n    ax2.set_ylabel(\"Normalized Intensity (0-1)\")\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    # ---------------------------------------------------------\n    # PLOT 3: Physical Correlation Heatmap\n    # Purpose: Which astronomical factor drives energy most?\n    # ---------------------------------------------------------\n    corr_cols = [TARGET] + FEATURES\n    corr_matrix = data[corr_cols].corr()\n    \n    ax3 = axes[1, 0]\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", ax=ax3)\n    ax3.set_title(\"3. Physics Correlation Matrix\")\n\n    # ---------------------------------------------------------\n    # PLOT 4: GHI vs DNI vs Energy (Scatter)\n    # Purpose: Does energy follow Direct (DNI) or Global (GHI) radiation?\n    # ---------------------------------------------------------\n    ax4 = axes[1, 1]\n    \n    # Filter daytime only for clearer plot\n    day_data = data[data['GHI'] > 10].sample(2000, random_state=42) # Sample to avoid lag\n    \n    sns.scatterplot(data=day_data, x='GHI', y=TARGET, ax=ax4, color='blue', alpha=0.3, label='GHI vs Energy')\n    sns.scatterplot(data=day_data, x='DNI', y=TARGET, ax=ax4, color='orange', alpha=0.3, label='DNI vs Energy')\n    \n    ax4.set_title(\"4. Irradiance Impact: GHI vs DNI\")\n    ax4.set_xlabel(\"Irradiance (W/m2)\")\n    ax4.set_ylabel(\"Energy Output\")\n    ax4.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # --- TEXT REPORT ---\n    print(\">>> ASTRONOMICAL INSIGHTS <<<\")\n    \n    # Seasonality Insight\n    max_month = monthly_stats[TARGET].idxmax()\n    print(f\"1. Peak Month: {max_month}\")\n    if max_month in [11, 12, 1, 2]:\n        print(\"   -> Insight: High production in Dec/Jan suggests SOUTHERN HEMISPHERE (Summer).\")\n    elif max_month in [6, 7, 8]:\n        print(\"   -> Insight: High production in Jun/Jul suggests NORTHERN HEMISPHERE (Summer).\")\n        \n    # Timezone Insight\n    print(f\"2. Solar Peak Hour: {peak_ghi_hour}:00\")\n    if peak_ghi_hour == 12:\n        print(\"   -> Insight: PERFECT! Solar Noon aligns with Clock Noon. No timezone lag.\")\n    elif peak_ghi_hour > 12:\n        print(f\"   -> Insight: LAG DETECTED. Sun peaks late. Location might be WEST of timezone center.\")\n    else:\n        print(f\"   -> Insight: EARLY PEAK. Sun peaks early. Location might be EAST of timezone center.\")\n\n# Execute\nastronomical_investigation(train_raw)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:06.737378Z","iopub.execute_input":"2025-11-23T12:37:06.737662Z","iopub.status.idle":"2025-11-23T12:37:08.408825Z","shell.execute_reply.started":"2025-11-23T12:37:06.737641Z","shell.execute_reply":"2025-11-23T12:37:08.407686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_monthly_distributions(df, target_col='% Baseline'):\n    data = df.copy()\n    data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n    data['Month'] = data['Timestamp'].dt.month\n    \n    # Mapping angka bulan ke nama biar cantik\n    month_map = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \n                 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n    data['Month_Name'] = data['Month'].map(month_map)\n    \n    # Urutan bulan yang benar\n    order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n             'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\n    # Setup Canvas\n    fig, axes = plt.subplots(3, 1, figsize=(14, 18))\n    \n    # --- 1. TARGET (% BASELINE) ---\n    sns.boxplot(data=data, x='Month_Name', y=target_col, order=order, ax=axes[0], palette='viridis')\n    axes[0].set_title('1. Monthly Energy Distribution (Target)', fontsize=14)\n    axes[0].set_ylabel('Energy Output (%)')\n    axes[0].grid(True, alpha=0.3, axis='y')\n    \n    # --- 2. GHI (GLOBAL HORIZONTAL IRRADIANCE) ---\n    sns.boxplot(data=data, x='Month_Name', y='GHI', order=order, ax=axes[1], palette='Oranges')\n    axes[1].set_title('2. Monthly Irradiance Distribution (GHI)', fontsize=14)\n    axes[1].set_ylabel('GHI (W/m¬≤)')\n    axes[1].grid(True, alpha=0.3, axis='y')\n\n    # --- 3. TEMPERATURE (TempC) ---\n    sns.boxplot(data=data, x='Month_Name', y='tempC', order=order, ax=axes[2], palette='coolwarm')\n    axes[2].set_title('3. Monthly Temperature Distribution', fontsize=14)\n    axes[2].set_ylabel('Temperature (¬∞C)')\n    axes[2].grid(True, alpha=0.3, axis='y')\n\n    plt.tight_layout()\n    plt.show()\n\n# Eksekusi\nplot_monthly_distributions(train_raw, target_col=TARGET)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:08.410145Z","iopub.execute_input":"2025-11-23T12:37:08.410417Z","iopub.status.idle":"2025-11-23T12:37:09.427035Z","shell.execute_reply.started":"2025-11-23T12:37:08.410398Z","shell.execute_reply":"2025-11-23T12:37:09.426253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inspect_physics_complete(df, sort_by='median'):\n    data = df.copy()\n    \n    # 1. Data Preparation\n    # Daytime only (GHI > 10) -> clouds at night don't matter\n    day_data = data[data['GHI'] > 10].copy()\n    \n    # High Irradiance only (GHI > 200) -> for stability in efficiency calculation\n    high_sun_data = data[data['GHI'] > 200].copy()\n    high_sun_data['raw_efficiency'] = high_sun_data['% Baseline'] / high_sun_data['GHI']\n    \n    # Create Canvas (1 row, 3 columns)\n    fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n    \n    # --- PLOT 1: TEMPERATURE VS EFFICIENCY (Thermal) ---\n    sns.scatterplot(\n        data=high_sun_data, \n        x='tempC', y='raw_efficiency', \n        alpha=0.1, ax=axes[0], color='crimson'\n    )\n    sns.regplot(\n        data=high_sun_data, \n        x='tempC', y='raw_efficiency', \n        scatter=False, ax=axes[0], color='black', line_kws={'linestyle':'--'}\n    )\n    axes[0].set_title('1. Thermal Physics: Temp vs Efficiency')\n    axes[0].set_xlabel('Temperature (¬∞C)')\n    axes[0].set_ylabel('Raw Efficiency')\n    axes[0].grid(True, alpha=0.3)\n\n    # --- PLOT 2: CLOUD COVER VS ENERGY (Opacity) ---\n    # Expectation: Negative Correlation. But does 100% cloud mean 0 energy?\n    sns.scatterplot(\n        data=day_data,\n        x='cloudcover', y='% Baseline',\n        alpha=0.05, ax=axes[1], color='dodgerblue' # low alpha to see density\n    )\n    # Add trend line\n    sns.regplot(\n        data=day_data,\n        x='cloudcover', y='% Baseline',\n        scatter=False, ax=axes[1], color='darkblue'\n    )\n    axes[1].set_title('2. Cloud Opacity: Coverage % vs Energy')\n    axes[1].set_xlabel('Cloud Cover (%)')\n    axes[1].set_ylabel('Energy Output (%)')\n    axes[1].grid(True, alpha=0.3)\n\n    # --- PLOT 3: CLOUD TYPE IMPACT (Categorical) ---\n    # Logic for sorting\n    if sort_by == 'Q3':\n        order_metric = day_data.groupby('Cloud Type')['% Baseline'].quantile(0.75).sort_values(ascending=False)\n    else:\n        order_metric = day_data.groupby('Cloud Type')['% Baseline'].median().sort_values(ascending=False)\n    \n    sns.boxplot(\n        data=day_data,\n        x='Cloud Type', y='% Baseline',\n        ax=axes[2], palette='Spectral',\n        order=order_metric.index.tolist()\n    )\n    axes[2].set_title(f'3. Cloud Type Impact (Sorted by {sort_by})')\n    axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=45, ha='right')\n    axes[2].grid(True, alpha=0.3, axis='y')\n\n    plt.tight_layout()\n    plt.show()\n\n# Execute\ninspect_physics_complete(train_raw, sort_by='median')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:10.425432Z","iopub.execute_input":"2025-11-23T12:37:10.425799Z","iopub.status.idle":"2025-11-23T12:37:12.150092Z","shell.execute_reply.started":"2025-11-23T12:37:10.425735Z","shell.execute_reply":"2025-11-23T12:37:12.149016Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"Data yang dilatih berupa data tabular dengan total fitur yang digunakan sebanyak 36 setelah dibersihkan. Karena kebanyakan fitur merupakan data numerik, maka akan digunakan model machine learning tradisional seperti XGBoost dan/atau LightGBM yang robust terhadap data numerikal dan outlier.","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"train_df = train_raw.copy()\ntest_df = test_raw.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:12.151098Z","iopub.execute_input":"2025-11-23T12:37:12.151353Z","iopub.status.idle":"2025-11-23T12:37:12.159204Z","shell.execute_reply.started":"2025-11-23T12:37:12.151334Z","shell.execute_reply":"2025-11-23T12:37:12.158389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_engineering(df):\n    df = df.copy()\n\n    # Standardize timestamp\n    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n    df = df.sort_values('Timestamp')\n\n    # --- CLOUD MAPPING ---\n    cloud_map = {\n        'Unknown': 0, 'Opaque Ice': 1, 'Overlapping': 2, 'Super-Cooled Water': 3, \n        'Cirrus': 4, 'Fog': 5, 'Water': 6, 'Overshooting': 7, \n        'Probably Clear': 8, 'Clear': 9\n    }\n    df['Cloud Type'] = df['Cloud Type'].map(cloud_map).fillna(0)\n\n    # --- PHYSICS CORRECTION (CRITICAL) ---\n    # Observation: Solar peak occurs at 17:00 in raw data.\n    # Adjustment: Shift time by -5 hours to align Solar Noon with 12:00.\n    # This helps the model understand the true \"shape\" of the solar day.\n    solar_time = df['Timestamp'] - pd.Timedelta(hours=5)\n    solar_hour = solar_time.dt.hour\n    doy = solar_time.dt.dayofyear\n\n    # --- CYCLICAL FEATURES ---\n    # Calculated on Solar Time for better alignment with physics\n    df['hour_sin'] = np.sin(2 * np.pi * solar_hour / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * solar_hour / 24)\n    \n    # Monthly cycles\n    df['month_sin'] = np.sin(2 * np.pi * df['Timestamp'].dt.month / 12)\n    df['month_cos'] = np.cos(2 * np.pi * df['Timestamp'].dt.month / 12)\n\n    # --- ASTRONOMICAL FEATURES ---\n    # Solar Declination (Cooper 1969)\n    delta = 23.45 * np.sin(np.radians(360 * (284 + doy) / 365))\n    df['solar_declination'] = delta\n\n    # Equation of Time (EoT)\n    B = np.radians((doy - 81) * 360 / 365)\n    df['equation_of_time'] = 9.87 * np.sin(2*B) - 7.53 * np.cos(B) - 1.5 * np.sin(B)\n\n    # Earth-Sun Distance Factor (Eccentricity)\n    df['sun_earth_distance_factor'] = 1 + 0.033 * np.cos(np.radians(360 * doy / 365))\n    df['extraterrestrial_radiation'] = 1367 * df['sun_earth_distance_factor']\n\n    # --- SUNRISE / SUNSET LOGIC ---\n    # Parsing raw time strings\n    sunrise_dt = pd.to_datetime(df['sunrise'], format='%I:%M %p')\n    sunset_dt = pd.to_datetime(df['sunset'], format='%I:%M %p')\n    \n    # Duration of daylight\n    df['sunHour'] = (sunset_dt - sunrise_dt).dt.total_seconds()\n    \n    # Accurate daytime flag (comparing raw times)\n    curr_time = df['Timestamp'].dt.time\n    rise_time = sunrise_dt.dt.time\n    set_time = sunset_dt.dt.time\n    \n    df['is_daytime'] = [\n        1 if (r <= c <= s) else 0 \n        for c, r, s in zip(curr_time, rise_time, set_time)\n    ]\n\n    # --- SOLAR PHYSICS RATIOS ---\n    # Adding epsilon to avoid division by zero\n    epsilon = 1e-6\n    df['clearsky_index'] = df['GHI'] / (df['Clearsky GHI'] + epsilon)\n    df['diffuse_fraction'] = df['DHI'] / (df['GHI'] + epsilon)\n    \n    # Wind Cooling Potential (using Kelvin)\n    df['wind_cooling_potential'] = df['windspeedKmph'] / (df['tempC'] + 273.15)\n\n    # --- TIME AWARE FEATURES (SMART MERGE) ---\n    # Create lag features correctly handling gaps/jumps in data\n    df['target_time_1h'] = df['Timestamp'] - pd.Timedelta(hours=1)\n    \n    lookup = df[['Timestamp', 'GHI', 'cloudcover']].copy()\n    lookup.columns = ['ts_ref', 'GHI_lag1', 'cloudcover_lag1']\n    \n    df = df.merge(lookup, left_on='target_time_1h', right_on='ts_ref', how='left')\n    \n    # Rolling stats (3h window)\n    indexer = df.set_index('Timestamp')\n    df['GHI_rolling_mean_3h'] = indexer['GHI'].rolling('3h', min_periods=1).mean().values\n\n    # Cleanup\n    df.drop(columns=['target_time_1h', 'ts_ref'], inplace=True)\n    \n    features_to_fill = ['GHI_lag1', 'cloudcover_lag1', 'GHI_rolling_mean_3h']\n    df[features_to_fill] = df[features_to_fill].fillna(0)\n    \n    return df\n\n# Apply to datasets\ntrain_df = feature_engineering(train_df)\ntest_df = feature_engineering(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:12.160026Z","iopub.execute_input":"2025-11-23T12:37:12.160256Z","iopub.status.idle":"2025-11-23T12:37:12.343227Z","shell.execute_reply.started":"2025-11-23T12:37:12.160236Z","shell.execute_reply":"2025-11-23T12:37:12.342391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:37:12.344066Z","iopub.execute_input":"2025-11-23T12:37:12.344308Z","iopub.status.idle":"2025-11-23T12:37:12.349290Z","shell.execute_reply.started":"2025-11-23T12:37:12.344288Z","shell.execute_reply":"2025-11-23T12:37:12.348558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Traditional Machine Learning","metadata":{}},{"cell_type":"code","source":"def get_feature_groups(\n    df, target_col, cyclical_cols, manual_drop_cols=None\n):\n    \"\"\"\n    Memisahkan kolom menjadi Numerical, Ordinal, dan Cyclical secara otomatis.\n    \"\"\"\n    if manual_drop_cols is None:\n        manual_drop_cols = ['tempC', 'DewPointC', 'Pressure'] # Default drop\n        \n    # Kolom yang pasti dibuang (Target + Cyclical mentah + Manual Drop)\n    features_to_drop = [target_col] + cyclical_cols + manual_drop_cols\n    \n    # Numerical: Ambil semua angka, lalu buang yang masuk daftar drop\n    num_features = df.select_dtypes(np.number).columns.difference(features_to_drop).tolist()\n    # Ordinal: Hardcoded sesuai strategimu\n    ord_features = ['Cloud Type']\n    # Cyclical: Sesuai input\n    cyc_features = cyclical_cols\n    \n    # Gabungkan semua untuk select X nanti\n    all_selected = list(set(num_features + ord_features + cyc_features))\n    \n    print(f\"üìä Features Summary:\")\n    print(f\"   - Numerical : {len(num_features)}\")\n    print(f\"   -   Ordinal : {len(ord_features)}\")\n    print(f\"   -  Cyclical : {len(cyc_features)}\")\n    print(f\"   -     TOTAL : {len(all_selected)}\")\n    \n    return num_features, ord_features, cyc_features, all_selected\n\ndef build_preprocessor(num_feat, ord_feat, cyc_feat):\n    \"\"\"\n    Menyusun ColumnTransformer agar rapi.\n    \"\"\"\n    # Sub-pipeline untuk Cyclical\n    cyclical_pipeline = Pipeline([\n        ('cyclic_encoding', SolarLunarTransformer(features=cyc_feat))\n    ])\n\n    # Sub-pipeline untuk Numerical\n    numerical_pipeline = Pipeline([\n        ('numeric_imputer', SimpleImputer(strategy='mean')),\n        ('numeric_scaler', StandardScaler())\n    ])\n\n    # Main Transformer\n    preprocessor = ColumnTransformer(transformers=[\n        ('ord', 'passthrough', ord_feat),\n        ('cyc', cyclical_pipeline, cyc_feat),\n        ('num', numerical_pipeline, num_feat),\n    ])\n    \n    return preprocessor\n\ndef get_model(model_name='xgb', random_state=42):\n    \"\"\"\n    Mengembalikan object model yang sudah dikonfigurasi parameternya.\n    \"\"\"\n    # --- LightGBM Config ---\n    lgbm_params = {\n        'n_estimators': 5000,\n        'learning_rate': 0.0325,\n        'num_leaves': 50,\n        'max_depth': -1,\n        'min_child_samples': 20,\n        'subsample': 0.65,\n        'colsample_bytree': 0.85,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'objective': 'regression',\n        'metric': 'rmse',\n        'random_state': random_state,\n        'n_jobs': 1,\n        'verbose': -1\n    }\n\n    # --- XGBoost Config ---\n    xgb_params = {\n        'n_estimators': 2600, \n        'learning_rate': 0.029244, \n        'max_depth': 8, \n        'min_child_weight': 6, \n        'subsample': 0.6568, \n        'colsample_bytree': 0.8655, \n        'reg_alpha': 0.00458,\n        'reg_lambda': 7.83e-05, \n        'n_jobs': 1, \n        'random_state': random_state,\n    }\n\n    if model_name == 'lgbm':\n        return LGBMRegressor(**lgbm_params)\n    elif model_name == 'xgb':\n        return XGBRegressor(**xgb_params)\n    else:\n        raise ValueError(\"Model not supported. Choose 'xgb' or 'lgbm'\")\n\n# Setup Data & Features\nnum_cols, ord_cols, cyc_cols, sel_features = get_feature_groups(\n    train_df, TARGET, solar_lunar\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:08:08.946940Z","iopub.execute_input":"2025-11-23T13:08:08.947257Z","iopub.status.idle":"2025-11-23T13:08:08.985498Z","shell.execute_reply.started":"2025-11-23T13:08:08.947236Z","shell.execute_reply":"2025-11-23T13:08:08.984440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_evaluate(\n    model_name, X, y, preprocessor, \n    n_splits=5, seed=42, use_sqrt_target=False\n):\n    \"\"\"\n    Fungsi eksekusi utama: Build Pipeline -> CV -> Print Result\n    \"\"\"\n    print('='*60)\n    print(f\"üöÄ TRAINING STARTED: {model_name.upper()}\")\n    print('='*60)\n    \n    # 1. Ambil Model\n    model = get_model(model_name, seed)\n    \n    # 2. Bungkus Model (Opsional: TransformedTargetRegressor untuk RMSE lebih stabil)\n    if use_sqrt_target:\n        # Kalau mau pakai teknik akar kuadrat target (Pipeline_sqrt)\n        regressor = TransformedTargetRegressor(\n            regressor=model,\n            func=np.sqrt,\n            inverse_func=np.square\n        )\n    else:\n        regressor = model\n\n    # 3. Buat Pipeline Akhir\n    final_pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', regressor)\n    ])\n    \n    # 4. Cross Validation\n    print(f\"Running {n_splits}-Fold CV...\")\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    \n    results = cross_validate(\n        final_pipeline, X, y, \n        cv=cv, \n        scoring='neg_mean_squared_error',\n        n_jobs=-1,\n        return_train_score=False\n    )\n    \n    # 5. Reporting\n    scores = -results['test_score'] # Convert negative RMSE to positive\n    \n    print(\"-\" * 60)\n    for i, score in enumerate(scores):\n        print(f\"  Fold {i+1} MSE: {score:.6f}\")\n    \n    print(\"-\" * 60)\n    print(f\"üèÜ {model_name.upper()} AVG MSE: {scores.mean():.6f} (+/- {scores.std():.6f})\")\n    print(\"-\" * 60)\n    \n    return final_pipeline, scores.mean()\n\nX = train_df[sel_features]\ny = train_df[TARGET]\n\n# Build Preprocessor\npreprocessor = build_preprocessor(num_cols, ord_cols, cyc_cols)\n\nwrapper = True\npipeline_xgb, score_xgb = train_and_evaluate(\n    'xgb', X, y, preprocessor, N_SPLITS, SEED, use_sqrt_target=wrapper)\n\npipeline_lgbm, score_lgbm = train_and_evaluate(\n    'lgbm', X, y, preprocessor, N_SPLITS, SEED, use_sqrt_target=wrapper)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:08:13.290926Z","iopub.execute_input":"2025-11-23T13:08:13.291262Z","iopub.status.idle":"2025-11-23T13:13:04.715496Z","shell.execute_reply.started":"2025-11-23T13:08:13.291239Z","shell.execute_reply":"2025-11-23T13:13:04.714636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# optuna.logging.set_verbosity(optuna.logging.ERROR)\n\n# def objective(trial):\n#     param_grid = {\n#         'n_estimators': trial.suggest_int('n_estimators', 1000, 3000, step=100),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n#         'max_depth': trial.suggest_int('max_depth', 5, 12),\n#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n#         'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n#         'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True), # L1 Regularization\n#         'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True), # L2 Regularization\n\n#         'tree_method': 'hist',\n#         'random_state': SEED,\n#         'n_jobs': -1,\n#         'device': 'cuda'\n#     }\n\n#     # Buat Model dengan parameter dari trial\n#     model = XGBRegressor(**param_grid)\n\n#     # Masukkan ke Pipeline\n#     pipeline_optuna = Pipeline(steps=[\n#         ('preprocessor', preprocessor),\n#         ('model', model)\n#     ])\n\n#     # Cross Validation\n#     cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n#     scores = cross_val_score(pipeline_optuna, X, y, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=1)\n    \n#     rmse = np.sqrt(-scores.mean())\n#     return -scores.mean()\n    \n# study = optuna.create_study(direction='minimize') # Minimize error\n# study.optimize(objective, n_trials=50, show_progress_bar=True)\n\n# print('Best hyperparameters:', study.best_params)\n# print('Best RMSE:', study.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T12:38:15.214708Z","iopub.status.idle":"2025-11-23T12:38:15.215036Z","shell.execute_reply.started":"2025-11-23T12:38:15.214887Z","shell.execute_reply":"2025-11-23T12:38:15.214900Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_importance_final(pipeline, ord_cols, cyc_cols, num_cols, top_n=25):\n    # --- 1. BUKA BUNGKUS MODEL (UNWRAPPER) ---\n    # Ambil step 'model' dari pipeline\n    wrapper = pipeline.named_steps['model']\n    \n    # Cek apakah dia TransformedTargetRegressor?\n    if hasattr(wrapper, 'regressor_'):\n        print(\"üì¶ Terdeteksi TransformedTargetRegressor. Mengambil inner model...\")\n        actual_model = wrapper.regressor_\n    else:\n        print(\"‚úÖ Model tidak dibungkus (Standard).\")\n        actual_model = wrapper\n        \n    # --- 2. AMBIL NILAI IMPORTANCE ---\n    if hasattr(actual_model, 'feature_importances_'):\n        # Sklearn standard / XGBoost Scikit-Learn API\n        importances = actual_model.feature_importances_\n    elif hasattr(actual_model, 'booster_'): \n        # LightGBM Native API\n        importances = actual_model.booster_.feature_importance(importance_type='gain')\n    else:\n        print(\"‚ùå Error: Model tidak memiliki atribut feature_importances_\")\n        return\n\n    # --- 3. SUSUN NAMA FITUR (SESUAI URUTAN PREPROCESSOR) ---\n    # Logika: Ordinal -> Cyclical (di-expand jadi sin/cos) -> Numerical\n    \n    # Expand Cyclical (karena pipeline cyclical memecah 1 kolom jadi 2)\n    expanded_cyc_feat = []\n    for feat in cyc_cols:\n        expanded_cyc_feat.append(f\"{feat}_sin\")\n        expanded_cyc_feat.append(f\"{feat}_cos\")\n    \n    # Gabungkan list nama\n    final_names = list(ord_cols) + expanded_cyc_feat + list(num_cols)\n    \n    # --- 4. VALIDASI & PLOTTING ---\n    print(f\"üìä Model Features: {len(importances)}\")\n    print(f\"üìù Feature Names : {len(final_names)}\")\n    \n    if len(final_names) != len(importances):\n        print(\"‚ö†Ô∏è Warning: Jumlah fitur tidak cocok! Menggunakan nama dummy.\")\n        final_names = [f\"Feature_{i}\" for i in range(len(importances))]\n    else:\n        print(\"‚úÖ MATCH! Nama fitur sinkron.\")\n\n    # Buat DataFrame\n    importance_df = pd.DataFrame({\n        'Feature': final_names,\n        'Importance': importances\n    })\n\n    # Sort & Plot\n    importance_df = importance_df.sort_values(by='Importance', ascending=False).head(top_n)\n\n    plt.figure(figsize=(10, 8))\n    sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\n    plt.title(f'Top {top_n} Feature Importance (Wrapper Supported)')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:13:13.898990Z","iopub.execute_input":"2025-11-23T13:13:13.899952Z","iopub.status.idle":"2025-11-23T13:13:13.910724Z","shell.execute_reply.started":"2025-11-23T13:13:13.899920Z","shell.execute_reply":"2025-11-23T13:13:13.909857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"‚è≥ Training ulang model pada seluruh dataset...\")\npipeline_xgb.fit(X, y) \nprint(\"‚úÖ Model selesai dilatih!\")\n\n# 2. BARU PLOT FEATURE IMPORTANCE\nplot_importance_final(\n    pipeline_xgb, \n    ord_cols, \n    cyc_cols, \n    num_cols\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:13:19.271143Z","iopub.execute_input":"2025-11-23T13:13:19.271489Z","iopub.status.idle":"2025-11-23T13:14:18.469210Z","shell.execute_reply.started":"2025-11-23T13:13:19.271466Z","shell.execute_reply":"2025-11-23T13:14:18.468190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"‚è≥ Training ulang model pada seluruh dataset...\")\npipeline_lgbm.fit(X, y) \nprint(\"‚úÖ Model selesai dilatih!\")\n\n\nplot_importance_final(\n    pipeline_lgbm,\n    ord_cols, \n    cyc_cols, \n    num_cols\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:14:18.470674Z","iopub.execute_input":"2025-11-23T13:14:18.471035Z","iopub.status.idle":"2025-11-23T13:15:08.863298Z","shell.execute_reply.started":"2025-11-23T13:14:18.471012Z","shell.execute_reply":"2025-11-23T13:15:08.862462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. PREDIKSI KE DATA TEST ---\nprint(\"\\nüîÆ Predicting Test Data...\")\n# Pastikan pakai data test yang sudah di-feature engineering\nX_test = test_df\n\npred_xgb = pipeline_xgb.predict(X_test)\npred_lgbm = pipeline_lgbm.predict(X_test)\n\n# --- 4. BLENDING (Weighted Average) ---\n# Kasih bobot 70% XGBoost (karena skornya udah dewa) + 30% LightGBM (buat jaga-jaga)\nprint(\"‚öñÔ∏è  Blending: 50% XGB + 50% LGBM\")\nfinal_pred = (0.45 * pred_xgb) + (0.55 * pred_lgbm)\n\n# Safety: Gak boleh negatif\nfinal_pred = np.maximum(final_pred, 0)\n\n# --- 5. MASKING MALAM HARI (Natural Sunrise/Sunset) ---\nprint(\"üåë Applying Night Masking...\")\n\n# Ambil jam sunrise/sunset dari data test\n# Copy dulu biar gak ngerusak data asli\ntemp_test = test_df.copy()\ntemp_test['Timestamp'] = pd.to_datetime(temp_test['Timestamp'])\n\n# Convert string jam ke object time\nsunrise_dt = pd.to_datetime(temp_test['sunrise'], format='%I:%M %p').dt.time\nsunset_dt = pd.to_datetime(temp_test['sunset'], format='%I:%M %p').dt.time\ncurrent_time = temp_test['Timestamp'].dt.time\n\n# Logika: Kalau jam skrg < sunrise ATAU jam skrg > sunset -> NOL\nis_night = [\n    (curr < rise) or (curr > set_) \n    for curr, rise, set_ in zip(current_time, sunrise_dt, sunset_dt)\n]\n\n# Eksekusi masking\n# final_pred[is_night] = 0\n\n# --- 6. SAVE SUBMISSION ---\nsubmission = pd.read_csv(SAMPLE_SUBMISSION)\nsubmission['% Baseline'] = final_pred\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"\\nFile saved in submission.csv\")\ndisplay(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:26:32.869584Z","iopub.execute_input":"2025-11-23T13:26:32.871498Z","iopub.status.idle":"2025-11-23T13:26:33.924260Z","shell.execute_reply.started":"2025-11-23T13:26:32.871465Z","shell.execute_reply":"2025-11-23T13:26:33.923216Z"}},"outputs":[],"execution_count":null}]}